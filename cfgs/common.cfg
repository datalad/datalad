# There should be a config file per repository
# identifying all pieces to be downloaded and placed in annexes:
#
#  incoming -- annex where we "download things" before either
#              linking or extracting into a public annex
#  public   -- annex where we extract and possibly complement
#              with manual additions/downloads.  But whatever gets
#              added automatically should be maintained in aggrement
#              with .meta_info.json stored in incoming

# By default incoming == public, but then we would track that things
# are not extracted?? TODO

# TODO: should we allow hierarchies? i.e. page pointing to files under
# different subdirectories and we would like to preserve that
# structure...

[DEFAULT]
# modes of operation.
#  download
#  fast   -- git annex addurl --fast, where no download will be
#            carried out, in case of encountering archives -- crash.
#            Intended for quickly creating git-annex repositories
#            for website directories such as arjlover.  In fast mode
#            it would not even query for remote filenames
mode = download
# BIG defaults which might need to be shared among ALL
# organizations and projects
keep_orig = True
# store "timestamp" information per each target file
meta_info = True
directory = %(__name__)s

# default
public = %(incoming)s

incoming_top = repos/incoming
public_top = repos/public

# Generic arrangement
incoming = %(incoming_top)s/%(organization)s/%(project)s
public = %(public_top)s/%(organization)s/%(project)s

# These are just common among sections
exclude_href =
# based on the link text
include_href_a =
exclude_href_a =


# specify which files should be extracted. How to extract would be
# decided internally
# By defaulit would just extract all known (internally) archives types
#archives_re = (\.(tar\.gz|tar\.bz))$

# It could also be used to add a suffix (e.g. %(filename)s.zip) corresponding
# to the archive whenever url/filename doesn't carry any
filename = %(filename)s




