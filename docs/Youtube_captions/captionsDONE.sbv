0:00:02.920,0:00:09.750
My name is Yaroslav Halchenko and I am talking to you from Dartmouth about DataLad project.

0:00:10.299,0:00:14.099
This project was initiated by me and Michael Hanke from Germany

0:00:14.099,0:00:21.028
and we had successful few years of collaboration. Before that you might know about us

0:00:22.480,0:00:24.599
because of our other projects such as PyMVPA and NeuroDebian.

0:00:25.140,0:00:30.129
I hope that you use them and they help you in your research projects.

0:00:30.129,0:00:32.660
More about these and other projects

0:00:32.660,0:00:37.680
You could discover if you go to the centerforopenneuroscience.org website, or you could also find

0:00:37.860,0:00:43.100
contacts for us in social media and before I proceed with the talk

0:00:43.110,0:00:49.650
I want first of all acknowledge work of others and the project. It wasn't only my and Michael's work

0:00:51.580,0:00:57.659
Our project is heavily based on Git-annex tool, which Joey Hess wrote for managing his own collection of files

0:00:58.060,0:01:00.269
which has nothing to do with science.

0:01:01.240,0:01:04.229
Also, he is well known for his work in Debian project

0:01:04.600,0:01:09.390
We had, we still have tireless workers on a project

0:01:09.909,0:01:11.020
Benjamin

0:01:11.020,0:01:16.920
working with Michael and Alex. Alex recently refurbished or wrote from scratch a new version of the website

0:01:16.920,0:01:20.249
I hope that you'll like it and we'll see a bit more of it later.

0:01:21.490,0:01:25.439
Also, we had Jason, Debanjum and Gergana working on the project.

0:01:26.469,0:01:30.299
They were quite successful to accomplish a lot within short period of time

0:01:31.119,0:01:37.199
So if you're looking for a project to contribute to it might be the interesting project for you to start

0:01:37.740,0:01:42.200
working on open source projects and leave in kind of your foot step in the

0:01:42.260,0:01:46.160
ecosystem of Open Source for Neuroscience.

0:01:46.160,0:01:49.920
This project is supported by NSF and

0:01:50.100,0:01:53.960
Federal Finistry for Education and Research in Germany.

0:01:54.400,0:02:00.160
If you go to centerforopenneuroscience.org you could discover more

0:02:00.380,0:02:04.500
interesting and exciting projects we either collaborate with it or contribute to.

0:02:06.369,0:02:12.239
Before we proceed I want actually to formulate the problem we are trying to solve DataLad.

0:02:12.970,0:02:18.499
Data is second class citizen within software platforms. What could that potentially be?

0:02:20.310,0:02:25.009
One of the aspects is if you look how people distribute data nowadays

0:02:25.710,0:02:32.239
Quite often you find that even large arrays of data are distributed in tarballs or zip files.

0:02:34.110,0:02:40.249
Problems were multiple with these ways of distribution if one file changes you need to re-distribute

0:02:40.820,0:02:48.120
Entire tarball which might be gigabytes in size, and that's why partially we also couldn't just adopt

0:02:48.720,0:02:50.130
technologies which are

0:02:50.130,0:02:54.840
Proven to work for software, let's say in Debian we distribute complete packages.

0:02:54.980,0:02:56.420
But again the problem is the same.

0:02:56.660,0:02:59.160
As long as you force

0:02:59.360,0:03:04.220
wrapping all the data together in some big file - it wouldn't work. It won't scale.

0:03:06.060,0:03:09.023
Also another problem is absent version of data.

0:03:09.023,0:03:15.060
And many people actually underappreciate it and think that it doesn't actually exist

0:03:15.060,0:03:21.780
or relates to their way of work. But no, actually this problem is quite generic.

0:03:22.920,0:03:24.920
So if you look into this

0:03:25.620,0:03:30.860
PhD comics caricature, you'll find that these probably relates to many

0:03:32.549,0:03:35.209
ways how you deal with files, data or

0:03:35.880,0:03:39.079
documents. And you could see that actually

0:03:39.930,0:03:47.600
how we tend to version our data is by providing - quite often - the date, right, which creates some kind of linear progression.

0:03:48.239,0:03:50.539
Right, so we annotate that: "Oh!"

0:03:51.540,0:03:56.209
"I've worked on these in those dates, but also maybe a little bit later..."

0:03:56.209,0:04:01.129
And we try to annotate it with some description of what was maybe done to the data

0:04:01.380,0:04:04.399
Right, so in this case. It was a test run

0:04:04.400,0:04:09.890
Then we test it again and calibrate it and then we ran into a problem, right? So...

0:04:10.470,0:04:16.519
All these, kind of, you saved the result of your work and annotated so later on you could

0:04:17.010,0:04:23.779
either get back to the previous state. Let's say maybe you indeed made an error and you want to rollback.

0:04:24.419,0:04:29.939
Or maybe you want to just compare what have you done, which broke your code or data?

0:04:30.700,0:04:34.770
Right, and as you could see those messages could be quite descriptive.

0:04:35.830,0:04:43.679
But the problem is that version control systems which are created for code are inadequate for data, right? So the problem is,

0:04:44.200,0:04:51.029
quite often, that it's duplication you have copy of the data in the version control system inside somewhere

0:04:51.030,0:04:52.450
so you couldn't use it directly.

0:04:52.450,0:04:57.440
But also it's present on your hard drive, so at least you have two copies quite often.

0:04:57.600,0:05:02.520
Or maybe it's duplicated and just on a single server, right?

0:05:03.010,0:05:09.330
I could give you examples were data in a version control system filled up the version control system and

0:05:09.940,0:05:12.480
meanwhile filling up the hard drive and

0:05:13.180,0:05:19.770
sometimes you try to commit new file and apparently ran out of space on the server and it might ruin your

0:05:20.110,0:05:22.319
version control back and then on the server

0:05:23.020,0:05:28.409
Rendering it impossible to get to the previous version, so you don't want to have that, right?

0:05:29.830,0:05:37.770
Then another problem is that there are no generic data distributions or at least there were no before DataLad.

0:05:38.110,0:05:40.410
So there is no efficient ways to

0:05:41.170,0:05:43.259
install and upgrade data sets and

0:05:44.740,0:05:52.109
When you also deal with different data hosting portals you need to learn how to navigate them

0:05:52.110,0:05:52.470
All right

0:05:52.470,0:05:53.830
you need to learn how you

0:05:53.830,0:06:00.149
authenticate, which page you need to go to and what to download and how to download it?

0:06:00.340,0:06:03.750
So just to get to that data set. And then, maybe you

0:06:05.230,0:06:08.129
get the announcement that dataset was fixed

0:06:08.130,0:06:14.309
and you need to repeat this over and over again trying to remember how to deal with it. And I'm not talking even if

0:06:14.920,0:06:22.379
the website became much better and sleeker and changed all the ways how it actually deals with downloads from what it did before.

0:06:23.980,0:06:27.029
Another aspect that data is rarely tested

0:06:27.160,0:06:29.999
So what does it mean for data to have bugs?

0:06:30.340,0:06:36.210
Any derived data is a product of running a script or some kind of procedure on

0:06:36.880,0:06:39.839
original data and generating new derived data.

0:06:40.990,0:06:44.939
Quite permanent ones which you could find in references later on in this presentation

0:06:45.460,0:06:52.079
is atlases. So Atlas is usually produced from the data writing some really sophisticated script

0:06:52.330,0:06:53.920
which generates new data:

0:06:53.920,0:06:56.819
the atlas. And those atlases could be buggy.

0:06:57.160,0:07:03.630
So how do you test the data? The same way as software. If we could establish this efficient process where we

0:07:04.630,0:07:08.999
produce some data and verify that at least data meets the assumptions

0:07:09.000,0:07:12.869
which you expect. If it's population or probability in the area

0:07:12.870,0:07:15.990
which must be present in the entirety of population,

0:07:16.060,0:07:21.300
then operability should be up to 100 or nearby 100. If it doesn't add up,

0:07:22.060,0:07:25.020
then you have a bug. It's really simple assumption

0:07:25.020,0:07:28.145
But very verifying that your data doesn't have...

0:07:28.145,0:07:30.779
Doesn't break those is really important.

0:07:32.140,0:07:38.729
Unified way how we deal with data, and the code could help to establish those data testing procedures.

0:07:39.580,0:07:46.619
Also, it's quite difficult to share your derived data. If downloaded some data set from an well known portal...

0:07:46.720,0:07:50.160
How do you share it? What data could be shared?

0:07:51.100,0:07:53.340
Where do you deposit it so people later on

0:07:53.830,0:07:59.819
Could download maybe original data, and your derive data without even worrying that oh

0:07:59.820,0:08:04.710
They need to get this piece from an original source and your derived data from another place

0:08:05.110,0:08:09.119
So how do we link those pieces together to make it convenient?

0:08:10.210,0:08:17.279
What we're trying to achieve is to make managing of the data as easy as managing code and software.

0:08:18.430,0:08:21.690
Is it possible? I hope that you'll see that it is so

0:08:22.300,0:08:23.350


0:08:23.350,0:08:25.350
What DataLad is based on...

0:08:25.780,0:08:31.380
Is in two pieces and one of them is Git. I hope that everybody knows what Git is.

0:08:31.900,0:08:33.010


0:08:33.010,0:08:38.369
But I'll give small presentation nevertheless. So Git is a version control system

0:08:38.650,0:08:46.259
and initially it was developed to manage Linux project code, if somebody doesn't know what Linux is this is one of the

0:08:46.840,0:08:54.599
most recognized and probably mostly used, because it's used everywhere: on the phones, on the servers, on the operating systems.

0:08:54.970,0:09:02.010
It's free and open source and it's developed into open and at some point they needed new version control system

0:09:02.010,0:09:07.739
Which would scale for the demand of having lots of code managed there and many people working with it

0:09:07.980,0:09:13.360
So it's not a geeky project just for... Between a few people.

0:09:13.900,0:09:16.000
It is developed by hundreds.

0:09:16.000,0:09:17.240
It's used by millions.

0:09:18.540,0:09:21.560
What's great about Git is that it's distributed.

0:09:21.780,0:09:27.440
So content is available across all copies of the repository if you clone the repository

0:09:28.000,0:09:30.960
You have the entire history of the project

0:09:30.960,0:09:35.840
and you could get to any point in that development you could compare different versions.

0:09:35.840,0:09:41.080
You could do exactly the same things as original developers dated on this repository.

0:09:41.080,0:09:46.900
So it provides you as much flexibility to accomplish things locally

0:09:47.470,0:09:50.460
without requiring any network access.

0:09:51.070,0:09:55.679
Git became a backbone for github and other social coding portals.

0:09:55.900,0:10:01.860
So github came to fill the niche that there were no convenient online resource

0:10:01.960,0:10:05.960
where people could easily share these repositories and work on them together.

0:10:06.480,0:10:12.820
So git is just a tool and github is just a web portal which provides you

0:10:13.000,0:10:20.720
a convenient centralized management of the repositories and collaboration between people.

0:10:20.740,0:10:23.220
But it's not a single one there are other

0:10:23.400,0:10:25.740
systems which use Git underneath.

0:10:26.400,0:10:29.740
Gitlab, Bitbucket, so...

0:10:31.000,0:10:37.320
It just creates this the entire ecosystem of the tool and additional services and resources.

0:10:38.280,0:10:48.120
What git is great for is very efficient management of textual information, right, so if you manage code, text, configuration files...

0:10:48.280,0:10:58.700
Maybe dumped some documentation or JSON files? So, all of those were nicely managed by git because it has really good mechanism to

0:10:58.940,0:11:02.662
annotate the differences and compress that efficiently.

0:11:02.662,0:11:07.000
So all those distributed copies are actually not that big.

0:11:07.200,0:11:10.560
But the problem or inefficiency of Git

0:11:10.820,0:11:17.020
is this exactly distributed nature of it. That it stores all the copies of the documents

0:11:17.320,0:11:23.400
on all the systems, right? So, if I have big files, then it becomes inefficient.

0:11:23.400,0:11:28.300
because now you will have two copies, right? You will have one on the hard drive (at least two copies)...

0:11:28.520,0:11:32.049
One on your hard drive and then one committed into Git.

0:11:32.050,0:11:35.620
Then if you push this into Github you will have again a

0:11:35.779,0:11:40.029
big copy of that file somewhere and anybody who clones that repository

0:11:40.580,0:11:43.900
might wait for a while to just get it and then

0:11:44.000,0:11:51.640
they might be a little bit upset because they wanted just one file from the repository and didn't care to download a gigabyte

0:11:52.120,0:11:54.180
of data just to see it.

0:11:54.180,0:11:56.640
So it's inefficient for storing data.

0:11:57.760,0:12:04.980
What is the other tool we rely on, as I said, written by Joey Hess it's Git-annex.

0:12:05.240,0:12:11.080
So the idea was to build on top of git to provide management for the data files

0:12:11.720,0:12:15.279
Without committing those files directly into Git.

0:12:16.520,0:12:22.449
So git-annex allows you to add data files under Git control without

0:12:23.120,0:12:26.770
committing the content of the files into Git.

0:12:27.589,0:12:33.748
While playing with Git-annex and DataLad you might see that files get replaced with the same link.

0:12:33.748,0:12:38.280
So what git-annex commits into Git is actually just symlink

0:12:38.280,0:12:41.780
which points to the file which contains the data.

0:12:42.170,0:12:48.130
This way you can commit really lightweight symlink and keep the data on the hard drive and a single

0:12:48.500,0:12:55.299
copy. Sorry, it's not in Git. And then what git-annex does, it orchestrate the

0:12:56.089,0:12:58.089
management of those files between

0:12:58.279,0:13:03.519
Different clones of the repository or so called other way special nodes.

0:13:03.800,0:13:10.630
But also it provides access to those files if they are let's say uploaded on to some website,

0:13:10.630,0:13:15.760
so you have a URL. You could associate the URL with the file, you could upload it to FTP,

0:13:16.100,0:13:18.820
you could upload it to web server.

0:13:19.550,0:13:25.839
You could even get content through BitTorrent, or you could use Amazon s3 storage as your

0:13:26.510,0:13:31.029
container for the files and it allows for custom extensions.

0:13:31.370,0:13:37.389
Let's say you could upload data to Dropbox, Google Drive, box.com and many, many other

0:13:38.839,0:13:40.958
data hosting provider.

0:13:42.079,0:13:46.929
Git-annex takes care also about avoiding the limitations of those platforms.

0:13:47.480,0:13:54.279
Let's say box.com from public account, it doesn't allow you to have files larger than I believe hundred megabytes.

0:13:54.889,0:14:00.489
Git-annex will chop it up so on the box.com you'll have little pieces

0:14:00.490,0:14:03.820
You will not use them directly from box.com, but then git-annex

0:14:03.820,0:14:09.129
will re-assemble the big file when it gets it onto your hard drive so all those

0:14:09.410,0:14:15.339
Conveniences and in addition encryption, that's if you want to share some sensitive data, and you cannot just upload it

0:14:16.130,0:14:20.079
Unencrypted to the public service all those are provided by git-annex.

0:14:20.839,0:14:27.789
Also additional feature which we don't use in a project is git-annex assistant which is Dropbox like

0:14:28.850,0:14:30.850
Synchronization mechanism you could establish

0:14:31.519,0:14:35.649
synchronization between your Git, git-annex repositories across multiple servers and

0:14:35.990,0:14:42.909
configure them really flexibly so you have that's a backup of on off all the data files on one server and

0:14:42.980,0:14:47.589
Some other server will have only files which it cares about let's say

0:14:47.930,0:14:52.299
Data files another one might have only video files

0:14:53.149,0:14:57.698
another one may be just music files who knows so flexibility is there and

0:14:58.060,0:15:02.400
It's all up to you to configure what you want where.

0:15:02.400,0:15:07.480
In our project we don't use it yet, but we do use it locally for synchronizing

0:15:07.700,0:15:10.020
different git-annex repositories.

0:15:12.380,0:15:15.309
But another problem here, so we have really

0:15:16.120,0:15:21.180
Great two tools Git and git-annex, but both of them work on a single repository level.

0:15:21.340,0:15:26.560
So, the work in a git repository you need to go into that directory and

0:15:27.320,0:15:33.400
Accomplish whatever you want to do. It kind of doesn't go along well with the notion of distribution

0:15:33.840,0:15:38.760
You don't care where you are you just want to, on your  hard drive, so you just want to say:

0:15:39.100,0:15:42.720
Oh, search, find me something, install this and

0:15:43.140,0:15:46.940
give me access to this data. Right? Or get me give me this file

0:15:47.080,0:15:50.700
Even though maybe I'm not in that git or git-annex repository

0:15:52.180,0:15:56.780
Also another kind of aspect those are just tools  so similarly like

0:15:57.120,0:16:02.820
how GitHub provided convenient portal to the tool git.

0:16:03.760,0:16:09.620
We want to accomplish something where we use these tools which are agnostic of domain of the data

0:16:09.630,0:16:16.410
(let's say neuroimaging) to give you guys access to those terabytes of publicly shared data already

0:16:16.720,0:16:21.569
Which lives out there somewhere, so we don't need to collect it. We don't need to

0:16:22.270,0:16:26.309
make copy of it locally, right, it's already there, so

0:16:26.950,0:16:31.890
What we want to achieve is just to provide access to that data without

0:16:32.230,0:16:36.180
Mirroring it on our servers or without duplicating it elsewhere

0:16:38.260,0:16:47.740
Before going into demos I want to give you kind of more illustrative demo of what is data lifecycle here of data

0:16:47.940,0:16:51.140
which we provide by DataLad.

0:16:51.340,0:16:59.180
Let's imagine that we have a data set which comes initially from OpenfMRI, right, and live somewhere in the cloud or

0:16:59.410,0:17:07.139
On data hosting portal actually we have two copies of the data one of them might be in the tarball, somewhere on HTTP server

0:17:07.420,0:17:09.420
right and another one might be

0:17:09.850,0:17:16.860
Extracted from the tarball, somewhere on a cloud which might have HTTP access might have S3 access,

0:17:16.860,0:17:18.900
but the point is that data is there and

0:17:19.480,0:17:25.980
Then we have a data user and that's us right me you everybody who wants to use this data

0:17:26.160,0:17:28.160
So now options are: we either...

0:17:28.390,0:17:34.680
Go down on the tarball extract it or we learn how to use S3 and go and install some tool

0:17:35.440,0:17:37.589
Browse S3 bucket, download those files.

0:17:38.950,0:17:42.510
But what we are trying to establish here is actually a middle layer, right?

0:17:42.710,0:17:48.499
We want to provide data distribution which might be hosted somewhere, maybe it's on github maybe in our server

0:17:49.050,0:17:55.310
Where I'll take this data available online and will automatically crawl it so here

0:17:55.310,0:18:02.600
I mentioned this command crawl which is one of the commands DataLad provides to automate monitoring of external resources

0:18:03.750,0:18:09.230
So we could get them into Git repositories and actually you could see here that these

0:18:11.040,0:18:12.750
Greenish-yellow

0:18:12.750,0:18:15.440
Why you don't draw here? Greenish yellow...

0:18:16.260,0:18:17.550
color.

0:18:17.550,0:18:19.550
Why you don't draw here?

0:18:20.580,0:18:27.919
Here we go! So, this greenish yellow color represents just a Content reference

0:18:28.620,0:18:34.280
Instead of the actual content, that's why we could host it on github or anywhere because it doesn't have the actual data

0:18:35.250,0:18:40.310
So we collect those data sets into collections, which we might share

0:18:40.310,0:18:44.929
let's save the one which we share from data sets that are on DataLad.org

0:18:45.330,0:18:51.080
underneath we use git modules which is built-in mechanism within Git to organize these collections of

0:18:51.270,0:18:55.340
multiple repositories while keeping track of burgeoning information

0:18:55.340,0:18:58.369
So you could get the entire collection of let's say of OpenfMRI data sets

0:18:58.560,0:19:02.749
For a specific date for a specific version if you want to reproduce some of these else analysis

0:19:02.750,0:19:06.140
And then we are making it possible to install

0:19:06.660,0:19:10.009
Arbitrary number of those data sets we are unified interface

0:19:10.710,0:19:16.639
So here we mentioned command datalad --install which you will see later and hopefully

0:19:17.400,0:19:22.400
Those parameters like install into current data set and get all the data

0:19:23.010,0:19:28.550
Will it be less surprising and also we provide shortcuts so which I'll talk about later

0:19:28.800,0:19:31.100
But the point is that you could now easily

0:19:31.680,0:19:36.680
Install those data sets onto your local hard drive, and if you are doing some processing

0:19:37.530,0:19:44.810
It might add results of the process in this case. We've got new file filtered bold file, which we could easily add

0:19:45.660,0:19:50.480
Into this repository and which means which is committed into the repository

0:19:51.000,0:19:58.739
Under git-annex control. And later we could publish this entirety of maybe collection of the datasets

0:20:00.010,0:20:05.010
to multiple places one of them might be github or we publish only the

0:20:06.130,0:20:08.729
repository itself without actually data files again

0:20:08.730,0:20:16.170
Those are just symlinks and maybe offload the actual data to some server which my HTTP server

0:20:18.100,0:20:25.230
or some other server through some mechanism right, but the point is that data goes somewhere and the magic happens here

0:20:25.330,0:20:31.709
Thanks to the git-annex because that's the Beast which keeps track of were each data file

0:20:32.080,0:20:36.929
Could be obtained from so this red links point to the information

0:20:36.930,0:20:44.339
What git-annex stores for us that I let's say this bald file is available from original web portal right it's available from S3 bucket,

0:20:44.340,0:20:48.300
it might be coming from a tarball, so that's one of the extensions

0:20:48.300,0:20:53.190
we added to git-annex to support extraction of the files from the tarball.

0:20:53.740,0:20:57.330
So it becomes really transparent to the user and this new file

0:20:58.570,0:21:04.889
We published it there. So it might be available now through HTTP so people who cloned this repository

0:21:06.220,0:21:13.620
Would be able to get any file from original storage or from any derived data

0:21:13.720,0:21:15.720
Which we published on our website?

0:21:16.840,0:21:20.250
So that's kind of the main idea behind DataLad.

0:21:21.610,0:21:23.260
So, altogether

0:21:23.260,0:21:28.650
DataLad allows you to manage multiple repositories organized into these super datasets

0:21:28.650,0:21:34.680
Which are just collection of git repositories using standard git sub-modules mechanism.

0:21:34.990,0:21:38.760
It supports both git and ggit-annex repository, so if you have

0:21:39.490,0:21:45.180
Just regular git repositories where you don't want to add any data. It's perfectly fine.

0:21:45.940,0:21:52.499
We can crawl external online data resources and update git-annex repositories upon changes.

0:21:53.770,0:21:59.160
It seems to scale quite nicely because data stays with original data provider

0:21:59.160,0:22:02.369
so we don't need to increase the storage in our server and

0:22:02.920,0:22:09.020
We could use maybe or you could use because anybody could use DataLad to publish

0:22:09.200,0:22:11.300
their collections of the datasets on

0:22:12.760,0:22:19.679
github and maybe offloading data itself to portals like box.com or dropbox.

0:22:21.279,0:22:25.769
What happens now that we have unified access to data regardless of its origin

0:22:25.770,0:22:30.389
I didn't care if data comes from openfMRI or CRCNS.

0:22:30.820,0:22:36.960
The only difference might be that you need to authenticate it. Let's say CRCNS doesn't allow download without authentication.

0:22:37.539,0:22:42.929
So DataLad will ask you for credentials, which you should store locally in the hard drive

0:22:42.960,0:22:46.649
Nothing is shared with us and later on when you need to get more data

0:22:46.750,0:22:51.089
Just to use those credentials to authenticate in your behalf to CRCNS,

0:22:51.640,0:22:55.559
download those tarballs extract it for you, so you didn't need to worry about that and

0:22:56.260,0:23:00.929
Also, we take care about serialization, so if original website distributes only tarballs

0:23:01.779,0:23:04.919
We download tarballs for you, extract them and again

0:23:04.919,0:23:08.939
You didn't need to worry how the data is actually serialized by original data provider

0:23:09.940,0:23:13.770
What we do on top is that we aggregate metadata.

0:23:14.320,0:23:18.390
What metadata is? It is data about the data.

0:23:18.880,0:23:24.779
So let's say you have a data set which contains the data the results information about what this data

0:23:24.779,0:23:28.409
Set is about what it's named. What was its offer authors?

0:23:29.080,0:23:31.640
What might be the license if it's applicable?

0:23:32.160,0:23:35.880
so any additional information about the data constitutes metadata.

0:23:36.140,0:23:42.080
What we do in DataLad, we aggregate metadata, which we find about the original data sets and

0:23:42.820,0:23:44.490
Provide you convenient interface

0:23:44.490,0:23:48.329
So you could search across all of it across all the data sets which we already

0:23:48.640,0:23:52.049
Integrated in DataLad. And I hope you'll see the demonstration

0:23:52.750,0:23:54.959
quite appealing later on.

0:23:56.049,0:24:01.679
Then DataLad after you consumed added extended data sets or just created from scratch

0:24:02.380,0:24:09.839
You could share original or derived datasets publicly as I mentioned or internally you could always

0:24:10.659,0:24:16.709
publish them locally at your SSH may be to collaborate with somebody and that's what we do regularly and

0:24:17.919,0:24:19.919
Meanwhile we'll keep data

0:24:20.320,0:24:27.780
we could keep data available elsewhere, or you could even share the data set without sharing the data, which is quite keen as

0:24:29.860,0:24:36.360
Demonstration of good intent when you are about to publish the paper, that's what we did them with our recent submission

0:24:36.360,0:24:40.829
We publish the data set but not with the entirety of the data set

0:24:40.830,0:24:45.449
But just with first subject so reviewers could verify that there is

0:24:46.450,0:24:48.130
Good quality data

0:24:48.130,0:24:49.990
that

0:24:49.990,0:24:56.099
They could get access to it right and that the entirety of data is in principle available

0:24:56.100,0:25:00.089
And it was processed accordingly because the whole the entire Git history

0:25:00.850,0:25:04.650
is maintained and shared, but the data files are not

0:25:06.160,0:25:10.560
Okay and the additional benefit some of it, which is work in progress

0:25:11.080,0:25:15.449
You could export the data set if you want to share just the data itself you could

0:25:15.580,0:25:21.420
Export the data set and current version in a tarball and give it to somebody but more exciting feature

0:25:21.700,0:25:25.680
and we've been working on is exporting in to

0:25:26.290,0:25:27.370
some

0:25:27.370,0:25:31.170
Metadata heavy data formats if you're publishing scientific data

0:25:31.660,0:25:37.170
You will be asked to fill out a big spreadsheet, which is called easy to have

0:25:38.950,0:25:44.340
To annotate metadata for your data set it's really tedious and unpleasant job

0:25:44.340,0:25:48.630
But the beauty is that all that information is contained within

0:25:49.030,0:25:54.180
metadata of either data set or of git-annex. So we could automatically

0:25:54.580,0:26:01.199
export majority of information for you, so you just need to fill out left out information and be done

0:26:04.150,0:26:07.680
DataLad comes with both common line and Python interfaces

0:26:07.680,0:26:14.489
So you could work with it interactively either in common line or script it in bash or working with it interactively in the ipython

0:26:14.830,0:26:20.100
and script it with Python language it gives you the same capabilities and really similar syntax

0:26:22.300,0:26:24.100
Our distribution

0:26:24.100,0:26:27.089
Grew up already to cover over ten terabytes of data

0:26:27.910,0:26:31.469
We cover such data sets as OpenfMRI, CRCNS,

0:26:32.560,0:26:34.560
functional connectome

0:26:34.870,0:26:42.430
INDI data sets and even some data sets from Kaggle and some RatHole radio podcast show

0:26:42.830,0:26:49.059
Because it was a cool experiment to be able to crawl that website and collect all the data

0:26:49.520,0:26:52.599
About timing of the songs. So check it out

0:26:52.600,0:26:57.100
It's available on github although data stays as again with original provider

0:26:57.370,0:27:03.609
What is coming? More data, so we'll cover human connectome project and data available from x net servers

0:27:03.890,0:27:08.410
We want to provide extended metadata support, so we cover not only data sets

0:27:08.410,0:27:09.190
level data

0:27:09.190,0:27:16.750
But also data for separate files if you know about any other interesting data set or data provider

0:27:17.390,0:27:20.739
File a new issue, or shoot us an email.

0:27:21.590,0:27:27.850
we are also working on integrating with NeuroDebian, so you could apt-get install those datasets and the position of data to

0:27:28.580,0:27:32.350
OSF and in other platforms. Another interesting integration

0:27:32.350,0:27:39.880
Which we've done was to introduce DataLad support into HeuDiConv which stands for Heuristic DICOM Conversion Tool.

0:27:39.880,0:27:41.809
which allows you to

0:27:41.809,0:27:47.739
automate conversion of your DICOM data obtained from MRI scanner into NIfTI files

0:27:48.080,0:27:51.520
but we went one step further and

0:27:52.280,0:27:57.489
Standardized it to convert not only to DataLad data set but DataLad BIDS data sets.

0:27:57.490,0:28:02.020
Set so if you don't know what BIDS is, it is something you must know nowadays.

0:28:02.540,0:28:07.479
If you doing imaging research. It's brain imaging data structure format

0:28:07.700,0:28:14.679
Which describes how you should lay out your files on a file system so anybody who finds your data set will be immediately

0:28:15.230,0:28:17.230
capable to understand

0:28:17.690,0:28:24.970
your design how many subjects you have so it's standardized is beyond NIfTI. It standardized is how you

0:28:25.280,0:28:27.280
Work with your files so now

0:28:27.680,0:28:33.500
With this integration HeuDiConv we can obtain DataLad datasets

0:28:34.360,0:28:39.660
with BIDS if I'd neuroimaging data, so it's ready to be shared

0:28:39.670,0:28:44.109
It's ready to be processed by any BIDS compatible tool, so it opens ample

0:28:44.660,0:28:46.660
opportunities

0:28:46.790,0:28:50.930
And at this point I guess we should switch and do some demos

0:28:53.640,0:28:59.989
And before I actually give any demo I want to familiarize you with our new website DataLad.org

0:29:01.110,0:29:03.000
On top you could see

0:29:03.000,0:29:06.619
navigation for among major portions the website

0:29:07.140,0:29:09.170
One of them is about page

0:29:09.870,0:29:13.910
Just describes the purpose of the DataLad and provides

0:29:14.580,0:29:18.379
information about funding agencies and involved institutions

0:29:20.010,0:29:22.010
Next link is "Get DataLad"

0:29:22.890,0:29:30.350
Which describes how to install the DataLad. The easiest installation is if you are using your Debian already.

0:29:30.600,0:29:38.280
Then it just apt-get install DataLad command or you could find it in package manager and install it within second

0:29:38.720,0:29:45.320
Alternatively, if you are on OS-X or any other operating system. Windows support is initial but it

0:29:46.230,0:29:49.190
Should work in the basic set of features

0:29:49.800,0:29:51.950
you have to install git-annex by

0:29:52.500,0:29:54.590
going to git-annex website and

0:29:56.850,0:29:58.850
Into install page

0:29:59.280,0:30:05.840
choosing the operating system of your choice and following the instructions there how to get it and

0:30:07.200,0:30:09.140
after you installed git-annex

0:30:09.140,0:30:15.499
You just need to install DataLad from Python package index through pip-install datalad command.

0:30:16.920,0:30:19.399
Next page is features page

0:30:19.410,0:30:27.379
Which is actually led to by those pretty boxes on the main page and this page will go through

0:30:27.840,0:30:29.840
later in greater detail

0:30:30.150,0:30:33.379
Another interesting page is Datasets which presents you our

0:30:34.110,0:30:39.200
ultimate official distribution which points to datasets.datalad.org

0:30:39.990,0:30:44.479
which is the collection of data sets which already pre crawled for you and

0:30:45.240,0:30:50.420
That were we provide those data sets like for Open fRI

0:30:51.510,0:30:53.340
CRCNS

0:30:53.340,0:30:55.290
ADHD and

0:30:55.290,0:30:56.940
many others

0:30:56.940,0:31:00.469
I will just briefly describe the features of these

0:31:00.990,0:31:07.819
Basic website and mention that the such websites if you have any HTTP server available somewhere

0:31:08.130,0:31:13.459
Maybe institution provides because you will not host the data actually here, or you don't have to

0:31:14.429,0:31:21.349
you could upload similar views of your data sets pretty much anywhere where you could host a website and

0:31:22.140,0:31:26.479
OpenfMRI, let's say we go to OpenfMRI, it lists all those data sets

0:31:26.480,0:31:31.459
which we crawled from OpenfMRI, you could see also immediately mentioning of the version

0:31:31.980,0:31:33.980
here and version goes

0:31:33.990,0:31:35.990
from

0:31:36.000,0:31:42.949
What version OpenfMRI gave it but also with additional indices pointing to exact commits

0:31:44.490,0:31:50.539
Within our git repository I didn't find that version another neat feature here is

0:31:51.389,0:31:52.919
immediate

0:31:52.919,0:31:57.469
Search so you could start typing and now if you're interested in resting-state

0:31:58.320,0:32:00.320
So here we go it goes

0:32:01.529,0:32:06.199
Pretty fast and limits the view only the data sets where metadata

0:32:07.350,0:32:12.169
Mentions this word and say let's look for Haxby... There we go!

0:32:12.779,0:32:18.619
Or let's look for "movie". There we go! So, you could quickly identify the data sets by

0:32:19.350,0:32:21.829
browsing and we'll see how we could do

0:32:22.289,0:32:25.039
such actions later in the command line and

0:32:25.289,0:32:31.638
When you get to the data set of interest or it could be at any pretty much level you'll see on top the command which

0:32:31.639,0:32:35.509
Could be used to install this data set and described in some options

0:32:35.510,0:32:40.969
Let's say -r is to install this data set with any possible sub data set recursively.

0:32:41.309,0:32:46.699
There's -g to install it and also obtain all the data for it, and if you want to speed up the

0:32:48.360,0:32:51.110
obtaining the data you could use -J

0:32:51.110,0:32:56.899
And specify the number of parallel downloads your server and bandwidth could allow you

0:32:57.929,0:33:01.788
Okay, let's go back to DataLad website and another

0:33:03.779,0:33:10.489
Page on the website is development. So, if you're interested to help and contribute datasets provide

0:33:11.039,0:33:13.039
patches improve documentation

0:33:13.320,0:33:20.140
All of the development this is made in open. We use github intensively, we use Travis, we use codecov.

0:33:20.920,0:33:27.200
We use Grid for documentation so and that will be our next point

0:33:28.509,0:33:33.479
Documentation is hosted on docs.datalad.org and it provides

0:33:34.720,0:33:39.480
Not yet as thorough documentation as we wanted but some

0:33:39.789,0:33:46.289
documentation about major features of the dataset or a comparison between Git, git-annex and DataLad.

0:33:46.659,0:33:48.659
But it also provides

0:33:49.179,0:33:56.429
Really thorough interface documentation so as I mentioned before we have command line and Python

0:33:57.519,0:34:01.199
interfaces both of those interfaces generated from the same code

0:34:01.200,0:34:03.539
So they should be pretty much identical

0:34:03.759,0:34:07.829
It just depending how you use command line or Python it will be different

0:34:07.839,0:34:11.129
But otherwise all the options all the commands

0:34:11.169,0:34:15.449
They look exactly the same and in command line reference

0:34:15.450,0:34:23.069
You could find all the documentation for all the commands you could use it that I have some popular ones in my case

0:34:23.069,0:34:29.099
Right where I went before and it provides documentation what those and of course there is

0:34:30.129,0:34:33.629
notes for power users and quite elaborate

0:34:34.179,0:34:37.648
documentation here about all the options which are available

0:34:38.230,0:34:40.230
in those commands

0:34:41.139,0:34:45.779
Ok so let's go back to features and

0:34:47.710,0:34:52.859
First of the demos which I want to show you will be about data discovery

0:34:54.399,0:34:59.608
That's any other demo on the website and is provided with

0:35:00.880,0:35:03.420
screencast which shows all

0:35:04.720,0:35:07.770
necessary commands to carry out the

0:35:09.670,0:35:12.119
Presentation, but also provides you with

0:35:13.210,0:35:17.399
comments describing the purpose of the actions taken

0:35:18.520,0:35:20.079
moreover

0:35:20.079,0:35:25.989
You could obtain the full script for the demo so you could run it as case on your hardware

0:35:28.040,0:35:32.379
By clicking underneath the screen screencast but

0:35:33.800,0:35:38.769
For this demonstration. I'll do it interactively in a shell together with you

0:35:40.280,0:35:41.960
So

0:35:41.960,0:35:43.960
Let's get started!

0:35:44.510,0:35:46.540
If as you remember

0:35:47.270,0:35:53.590
We aggregate a lot of metadata in DataLad to provide efficient search mechanisms

0:35:55.520,0:35:59.919
In this example we'll imagine that we were looking for a data set which mentions

0:36:00.650,0:36:07.600
Raiders in his word after being associated with movie Raiders of the Lost Ark and during imaging

0:36:09.230,0:36:12.399
So we'll use datalad -search command where we'll

0:36:13.430,0:36:16.300
Just state it right, so we'll call datalad -search

0:36:16.300,0:36:22.449
Raiders neuroimaging as with a mini or all commands in DataLad

0:36:23.060,0:36:26.110
They are composed by calling datalad

0:36:26.110,0:36:33.819
then typing the command you want to implement right and then you could ask for help for that command

0:36:36.440,0:36:37.850
Which

0:36:37.850,0:36:39.850
provides you with

0:36:39.950,0:36:41.950
associated help and

0:36:42.170,0:36:47.740
On my screen took a little bit longer just because of video recording usually it's a little bit faster like

0:36:48.380,0:36:50.359
five times and

0:36:50.359,0:36:54.369
Then you actually type the parameters for this command. For search

0:36:54.369,0:36:58.869
It's actually search terms, and I'll present a few other options later on

0:36:59.780,0:37:01.780
whenever you

0:37:02.270,0:37:09.759
Start this command for the first time it will ask you to install our super data set

0:37:11.210,0:37:17.590
Under your home DataLad, in my case that slash demo is the home directory so it asks either

0:37:17.590,0:37:21.609
We want to install that super dates which you saw available on DataLad.org

0:37:22.310,0:37:24.459
in your home directory.

0:37:24.460,0:37:32.139
And that's what it's doing so it quickly installed it because it's just a small git repository without any of those data sets

0:37:32.720,0:37:38.970
Directly in a part of it, but they are linked to it as sub modules. It was really fast, and then it loads and caches

0:37:39.610,0:37:44.069
metadata, which became available in that dataset and that takes few seconds

0:37:51.010,0:37:58.649
Whenever that is done it you see that by default it just returns the paths or names of the

0:38:00.190,0:38:05.369
Datasets as they are within the hierarchy of our super dataset and

0:38:07.510,0:38:09.510
Search searches within the

0:38:10.030,0:38:13.709
Repository data set you are in so if next time

0:38:13.710,0:38:20.159
I am just running the same command it will ask me instead of: "Oh, do you want to install it?" it'll ask me either

0:38:20.160,0:38:23.879
I want to search in this super dataset which I installed in my home directory

0:38:26.530,0:38:28.530
Type yes

0:38:32.710,0:38:37.619
And it provides the same result so to avoid such interactive questions

0:38:37.620,0:38:42.780
you could explicitly mention which data set you want to search in.

0:38:43.060,0:38:45.840
In our case it will be, I'll just specify

0:38:46.570,0:38:49.170
That data set will be this

0:38:49.930,0:38:51.400
canonical

0:38:51.400,0:38:54.270
DataLad data set which is installed in your

0:38:55.210,0:38:57.750
DataLad directory when you specify it like this

0:38:57.750,0:39:06.160
It assumes location in your home directory when you use triple slashes resource identifier as the source for URLs

0:39:06.360,0:39:14.020
To install data sets then it will go to the datasets.datalad.org. And this time we'll search not for Raiders

0:39:14.040,0:39:20.190
neuroimaging, but we'll search for Haxby, one of the authors within this data set

0:39:20.950,0:39:27.839
So -s stands for the fields which we want to search through and -R will report now

0:39:27.840,0:39:30.299
Not just the path to the data set but also

0:39:30.940,0:39:32.940
list the fields which match the

0:39:33.610,0:39:39.120
Query which we ran. So in this case it should search for data sets and report the field "author".

0:39:40.210,0:39:43.919
And only the data sets where Haxby was one of the authors.

0:39:44.620,0:39:46.390
So here they are

0:39:46.390,0:39:49.410
For convenience, let's just switch to that directory

0:39:50.160,0:39:52.160
under our home

0:39:52.329,0:39:56.939
Let me clear the screen and go to that directory

0:39:58.000,0:40:00.299
So now we don't have to specify

0:40:01.539,0:40:08.669
Location of the data set explicitly, and we could just type the same query without -d and it will provide the same results

0:40:13.420,0:40:19.200
Instead of listing all matching fields, let's say in our case it was "author" field, we could

0:40:20.019,0:40:25.409
Explicitly specify which fields you want to search through or to report.

0:40:26.289,0:40:31.319
So in this case, I want to see what's the name of the dataset and what is the author of the dataset?

0:40:31.319,0:40:33.989
It's already the author, but with didn't see the name.

0:40:35.109,0:40:39.929
And you're on the command to get the output with those fields included

0:40:41.710,0:40:43.630
Well enough of searching

0:40:43.630,0:40:47.369
Let's clear the screen and what we could do now we found

0:40:47.680,0:40:51.539
The data sets right it seems to be that the list of data sets which we found

0:40:52.360,0:40:55.860
is good to be installed and we could just

0:40:56.760,0:41:05.680
rely on a paradigm of Linux where you compose commands together through by using pipe command.

0:41:05.860,0:41:08.900
So, what this magic would do?

0:41:08.900,0:41:16.560
If we didn't have these which already what happens right so we get only the list of data sets or past

0:41:16.690,0:41:24.569
Those which are not installed yet, and so OpenfMRI directory is still empty so we get the list of data sets

0:41:25.320,0:41:29.060
But then instead of manually going and doing:

0:41:29.060,0:41:33.800
"datalad install openfmri/ds00233"...

0:41:33.980,0:41:39.060
Or doing copy-paste, we could just say that result of this command

0:41:39.400,0:41:41.400
should be passed as

0:41:41.400,0:41:45.760
Arguments to the next command which will be "datalad install".

0:41:45.760,0:41:51.140
"datalad install" command installs those data sets which are either specified by the

0:41:51.819,0:41:56.069
path within current data set or you could provide URLs to

0:41:56.800,0:42:02.300
Install command and it will go to those websites and download them explicitly from there.

0:42:02.300,0:42:05.780
"datalad install" could be used with other resources

0:42:06.340,0:42:10.800
beyond our canonical DataLad distribution.

0:42:11.140,0:42:13.140
So let's run this command

0:42:14.960,0:42:18.640
as a result of it you'll see that now it goes online and

0:42:19.550,0:42:25.630
Installs all those data sets or git/git-annex repositories without any data yet

0:42:25.670,0:42:29.950
So only the files which are committed directly into git will be present.

0:42:42.040,0:42:47.320
And now we could explore what actually we have got here.

0:42:47.320,0:42:52.200
I'll use another DataLad command. Let me clear the screen to bring it on top of the screen.

0:42:53.079,0:42:56.099
Next command is "ls", which just lists

0:42:56.940,0:43:01.400
either data sets or it could be used also at list S3 URLs.

0:43:01.400,0:43:04.380
If you are interested to see what is available in S3 bucket.

0:43:04.380,0:43:11.159
And we are specifying the options: capital -L for long listing, and -r recursively

0:43:11.160,0:43:15.780
So it will go through all data sets locally in current directory.

0:43:15.780,0:43:20.060
(That's why there is a period). And then we'll just remove a list in our data sets

0:43:20.069,0:43:23.129
which are not installed because they are not of our interest here.

0:43:56.050,0:44:01.889
As you can see all those datasets, which we initially searched for and found

0:44:03.820,0:44:05.820
Right?

0:44:09.850,0:44:13.860
They became installed, so they became available on our

0:44:14.500,0:44:19.290
Local file system and "ls" gives us idea. What kind of repository it is

0:44:19.530,0:44:21.870
It's Git versus annex, which branch it is in...

0:44:22.320,0:44:24.560
What was the date of the last commit?

0:44:24.920,0:44:33.360
Also, the sizes what it tells here that we have a lot of 4 gigabytes of data referenced in this data set at the current

0:44:33.900,0:44:39.340
version we've got only 0 bytes locally installed.

0:44:39.520,0:44:44.720
We installed only those symlinks I was talking about.

0:44:46.150,0:44:51.570
So, now we could actually explore what have you got?

0:44:53.290,0:44:58.709
Some of the files that were committed directly into Git, so they became available on the file system as is

0:44:59.320,0:45:06.029
But data files we could obtain now using the "datalad get" command.

0:45:06.670,0:45:09.629
So what this command will do... Let me clear the screen again...

0:45:10.440,0:45:16.960
So you're saying: "Obtain those files! Do it in four parallel processes."

0:45:16.960,0:45:24.040
All the files which match these Shell globe expressions,

0:45:24.040,0:45:26.480
so all the data sets locally which we have

0:45:26.860,0:45:32.759
For all the subjects underneath and anatomical directory, right? We obtained all ready two OpenfMRI dataset

0:45:32.760,0:45:35.040
And now we just want to obtain those data files

0:45:35.320,0:45:42.780
Let's actually see what this one is pointing to... It points to all those data files.

0:45:42.940,0:45:46.240
And if only listed with long listing,

0:45:46.240,0:45:51.480
we'll see that those were symlinks which are actually at the moment not even present on that

0:45:51.820,0:45:56.759
Point into the files which we don't have locally and that's what git-annex would do for us

0:45:56.760,0:45:59.760
It would go online and fetch all those files

0:46:00.910,0:46:02.910
from wherever they are available

0:46:03.520,0:46:05.520
So let me run this command now

0:46:18.220,0:46:21.629
As you can see the are four processes going on

0:46:27.640,0:46:29.640
And the end.

0:46:30.120,0:46:37.060
All DataLad commands they provide you a summary of what actions did it take?

0:46:37.280,0:46:42.020
Here you could see that it got all those files ready to get okay

0:46:42.299,0:46:49.619
Or it might say get failed if it failed to get them and then provides action summary, which we might see later in other demos

0:46:50.650,0:46:57.150
So let's now run the same command which you ran before to see how much of data we actually got?

0:47:26.540,0:47:30.199
As you can see all those which we didn't ask for any data

0:47:30.200,0:47:33.500
They still keep zero bytes although all

0:47:33.810,0:47:37.969
the files that are available and we could browse them,

0:47:37.970,0:47:42.830
but those where we requested additional data files to be obtained finally list how much data

0:47:42.830,0:47:46.159
we have in the working tree

0:47:47.070,0:47:50.059
of those data sets. 

0:47:51.120,0:47:54.739
That would complete the demo for "search" and "install".

0:47:56.610,0:48:01.489
Now it's your turn to find some interesting for you data sets and get the data for them

0:48:03.330,0:48:10.009
Now that we went through one of the demos on our website or we call it features which was data discovery

0:48:10.410,0:48:12.860
You could go and visit other

0:48:15.450,0:48:19.010
features described on this page. First one is for data consumers

0:48:19.010,0:48:25.580
which describes how you could generate native DataLad datasets from the website or

0:48:26.250,0:48:29.540
S3 buckets using our crawler so

0:48:30.870,0:48:33.409
If you know some resource you could create your own

0:48:33.870,0:48:40.999
DataLad crawler to obtain that data into DataLad dataset and keep it up to date with periodic reruns.

0:48:41.760,0:48:43.760
Data sharing demo will later show

0:48:44.970,0:48:50.570
examples of how you could share the data either on Github, through the github while depositing data to your website,

0:48:51.090,0:48:56.840
how I demonstrated earlier, or just for collaboration through SSH servers.

0:48:57.870,0:48:59.600
For Git and git-annex users

0:48:59.600,0:49:07.579
We give a little example of unique features present in DataLad contrasting it with

0:49:08.310,0:49:10.881
regular Git and git-annex usage.

0:49:10.881,0:49:16.280
This table outlines there those features.

0:49:16.280,0:49:23.740
We operate on multiple data sets at the same time, we operate across data sets seamlessly

0:49:23.750,0:49:30.169
So you don't have to switch directories to just operate in with specific data files they provide metadata support

0:49:30.860,0:49:37.840
And aggregate from different panel data sources and in unified authentication interface.

0:49:38.480,0:49:40.480
Also, one of the

0:49:40.700,0:49:48.159
new unique features in DataLad is ability to rerun previously ran commands on the data to see how

0:49:49.820,0:49:52.299
things changed or just keep nice

0:49:53.060,0:49:58.509
Protocol of actions you have done and record them within your git/git-annex history.

0:50:00.350,0:50:07.539
And the last one goes in detail in example on how to use HeuDiCon with your data sets and

0:50:07.730,0:50:09.730
relying on our

0:50:10.100,0:50:14.559
naming convention for how to name scanning sequences in the scanner.

0:50:15.640,0:50:18.540
I hope that you liked this presentation

0:50:18.540,0:50:27.700
and you liked what DataLad has to offer so I just want to summarize what DataLad does.

0:50:27.700,0:50:30.300
And what it does? It helps to manage and share

0:50:30.380,0:50:34.300
Available and your own data by a simple command line of Python interface.

0:50:34.880,0:50:38.530
We provide already access to over 10 terabytes of neuroimaging data

0:50:38.530,0:50:46.269
And we help with authentication, crawling of the websites, getting data from the archives in which it was originally distributed

0:50:47.000,0:50:49.000
publishing new or derived data.

0:50:50.000,0:50:55.449
Underneath we use regular pure Git and git-annex repository so whatever tools

0:50:55.450,0:50:58.780
You've got used to use you could still use them

0:50:58.780,0:51:01.810
And if you're an expert git and git-annex user

0:51:02.210,0:51:03.880
We will not limit your powers

0:51:03.880,0:51:11.800
You could do the same stuff what you did before with your key tanga tanga suppositories, so we also provide somewhat human

0:51:12.380,0:51:14.360
accessible

0:51:14.360,0:51:22.060
Metadata interface so in general if you want just to search for some datasets, it's quite convenient with datalad -search.

0:51:23.240,0:51:25.070
Documentation is growing

0:51:25.070,0:51:28.389
You're welcome to contribute, the project is open source.

0:51:29.240,0:51:36.790
I hope that after you've seen the presentation you will agree that managing data can be as simple as manage encode and software. Thank you!